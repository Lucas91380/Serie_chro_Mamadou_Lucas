---
title: "TP4, séries temporelles"
author: "Mamadou Lamine Diamban, Lucas Chabeau"
date: "12/11/2019"
output:
    pdf_document:
        latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library("astsa")

rm(list = ls())
options(OutDec = ",")
# Monthly Sales of U.S. Houses (in thousands of units),
# January 1965 to December 1975
sales <- c(38, 44, 53, 49, 54, 57, 51, 58, 48, 44, 42, 37,
           42, 43, 53, 49, 49, 40, 40, 36, 29, 31, 26, 23,
           29, 32, 41, 44, 49, 47, 46, 47, 43, 45, 34, 31,
           35, 43, 46, 46, 43, 41, 44, 47, 41, 40, 32, 32,
           34, 40, 43, 42, 43, 44, 39, 40, 33, 32, 31, 28,
           34, 29, 36, 42, 43, 44, 44, 48, 45, 44, 40, 37,
           45, 49, 62, 62, 58, 59, 64, 62, 50, 52, 50, 44,
           51, 56, 60, 65, 64, 63, 63, 72, 61, 65, 51, 47,
           54, 58, 66, 63, 64, 60, 53, 52, 44, 40, 36, 28,
           36, 42, 53, 53, 55, 48, 47, 43, 39, 33, 30, 23,
           29, 33, 44, 54, 56, 51, 51, 53, 45, 45, 44, 38)
# Monthly U.S. Housing Starts of Privately Owned Single-Family Structures (in thousands of units),
#January 1965 to Dcccmber 1975, ii
starts <- c(52.149, 47.205, 82.150, 100.931, 98.408, 97.351,
            96.489, 88.830, 80.876, 85.750, 72.351, 61.198,
            46.561, 50.361, 83.236, 94.343, 84.748, 79.828,
            69.068, 69.362, 59.404, 53.530, 50.212, 37.972,
            40.157, 40.274, 66.592, 79.839, 87.341, 87.594,
            82.344, 83.712, 78.194, 81.704, 69.088, 47.026,
            45.234, 55.431, 79.325, 97.983, 86.806, 81.424,
            86.398, 82.522, 80.078, 85.560, 64.819, 53.847,
            51.300, 47.909, 71.941, 84.982, 91.301, 82.741,
            73.523, 69.465, 71.504, 68.039, 55.069, 42.827,
            33.363, 41.367, 61.879, 73.835, 74.848, 83.007,
            75.461, 77.291, 75.961, 79.393, 67.443, 69.041,
            54.856, 58.287, 91.584, 116.013, 115.627, 116.946,
            107.747, 111.663, 102.149, 102.882, 92.904, 80.362,
            76.185, 76.306, 111.358, 119.840, 135.167, 131.870,
            119.078, 131.324, 120.491, 116.990, 97.428, 73.195,
            77.105, 73.560, 105.136, 120.453, 131.643, 114.822,
            114.746, 106.806, 85.504, 86.004, 70.488, 46.767,
            43.292, 57.593, 76.946, 102.237, 96.340, 99.318,
            90.715, 79.782, 73.443, 69.460, 57.898, 41.041,
            39.791, 39.959, 62.498, 77.777, 92.782, 90.284,
            92.782, 90.655, 84.517, 93.826, 71.646, 55.650)

sales <- ts(sales,start=c(1965,1),frequency=12)
starts <- ts(starts,start=c(1965,1),frequency=12)
# Voir les diapos 4 à 7 du document Housing.pdf
# Estimation des paramètres
model1 <- arima(sales,order=c(0,1,1),seasonal=list(order=c(0,1,1),period=12),method="CSS")
model1
# Récupération des valeurs numériques des paramètres estimés
theta <- model1$coef[1]
Theta <- model1$coef[2]
# On travaille avec les séries différenciées
dxt <- diff(diff(sales,lag=12))
dyt <- diff(diff(starts,lag=12))
acf(dxt)
acf(dyt)
# Filtrage de la série en entrée (sales)
at <- filter(dxt, filter=c(rep(0,11),-Theta),method="recursive",init=rep(0,12))
at <- filter(at, filter=c(-theta),method="recursive")
# Filtrage de la série en sortie (chantier)
bt <- filter(dyt, filter=c(rep(0,11),-Theta),method="recursive",init=rep(0,12))
bt <- filter(bt, filter=c(-theta),method="recursive")
# Corrélation entre les deux séries filtrées (17 de lag de chaque côté + la valeur du milieu = 35 valeurs)
crossCorr <- ccf(at,bt,lag.max = 17)
abline(v=0,lty=3,col="blue")
# Estimation des poids v[k] de la fonction de transfert.
temp <- crossCorr$acf * sd(bt) / sd(at) 
#cbind(crossCorr$acf,temp)
```

# 2 Modèle 2 : ventes de maisons neuves et mises en chantier.

En cours, nous avons étudié le modèle de fonction de transfert. Nous avons vu deux exemples détaillés. Le second voulait établir le lien entre le nombre de ventes de maisons neuves et le nombre de mises en chantier. Nous avons suggéré un premier modèle :

$$\nabla \nabla_{12} Y_t = \frac{0.8015B}{1 - 0.4005B}\nabla\nabla_{12}X_t + (1 − 0,6304B)(1 - 0.7849B^{12})\varepsilon_t$$

Cependant, nous avions des réserves car le corrélogramme croisé entre la série en entrée et les résidus du modèle final présentait une corrélation significativement différente de 0 au délai 0. Nous avons donc suggéré de reprendre le travail afin d’estimer le modèle :

$$\nabla \nabla_{12} Y_t = \frac{\omega_0 + \omega_1 B}{1 - \delta B}\nabla\nabla_{12}X_t + N_t \\$$

où $N_t$ est un processus ARMA. . .

Faites une copie du fichier Scripts/Housing1.R et renommer cette copie Scripts/Housing2.R. Modifier ce fichier à partir de la ligne 69 afin d’estimer et valider le modèle (2). Cela exigera de vous d’estimer à partir de l’estimation des $v_k$ les paramètres $\omega_0$, $\omega_1$ et $\delta$. Par la suite, procéder aux filtrages appropriés pour compléter le travail.

Nous rappelons cette formule générique pour estimer $v_k$

$$
v_k = \left \{ 
  \begin{array}{ll}
      0 & k\in \{0;...;b-1\} \\
      \sum_{j=1}^r \delta_j v_{k-j} + \omega_0 & k = b \\
      \sum_{j=1}^r \delta_j v_{k-j} - \omega_{k-b} & k\in \{b+1;...;b+s\} \\
      \sum_{j=1}^r \delta_j v_{k-j} & k \geq b+s+1 \\
  \end{array} 
\right .
$$

Dans notre modèle, nous avons $s = 0$, $r = 1$ et $b = 0$. Nous pouvons ainsi en déduire la relation suivante :

$$
v_k = \left \{ 
  \begin{array}{ll}
      \omega_0 & k = 0 \\
      \delta_1 v_0 - \omega_1 & k = 1 \\
      \delta_1 v_{k-1} & k \geq 2 \\
  \end{array} 
\right .
$$

A partir de là nous obtenons les estimateurs suivants :

$$
\left \{ 
  \begin{array}{l}
      \omega_0 = \hat{v}_0 \\
      \delta_1 = \frac{\hat{v}_2}{\hat{v}_1} \\
      \omega_1 = \delta_1 \hat{v}_0 - \hat{v}_1 \\
  \end{array} 
\right .
$$

```{r}
# Estimation des paramètres omega_0 et delta de la fonction de transfert. 
# Voir diapo 35 du fichier transfert.pdf
omega_0 <- temp[18]
delta_1 <- temp[16] / temp[17]
omega_1 <- delta_1 * temp[18] - temp[17]
```

A partir de l'estimation des $\hat{v}_k$, nous obtenons donc les estimations suivantes :

$\hat{\omega}_0$ = `r round(omega_0,4)`, $\hat{\omega}_1$ = `r round(omega_1,4)` et $\hat{\delta}_1$ = `r round(delta_1,4)`

Nous allons maintenant pouvoir récupérer la série $N_t$ et identifier son processus :

$$N_t = \nabla \nabla_{12} Y_t - \frac{\omega_0 + \omega_1 B}{1 - \delta B}\nabla\nabla_{12}X_t \\$$

Nous traçons ci-dessous les autocorrélogrammes complet et partiel de la série des bruits $N_t$.

```{r}
# Calcul du bruit qui sera modélisé par un processus ARMA
temp_t <- filter(filter(dxt, filter=c(omega_0,omega_1), method='convolution'), filter=c(delta_1), method='recursive')
Nt <- na.omit(dyt - temp_t)
# Identification du processus
par(mfrow=c(1, 2))
acf(Nt, ci.type = "ma",lag.max=36)
pacf(Nt,lag.max=36)
```

A la vue de ces autocorrélogrammes, nous identifions un processus saisonnier MA (Moving Average) pour les raisons suivantes :

- L'autocorrélogramme simple nous montre un nombre fini de corrélations significativement différentes de zéro, et aucune ne l'est après le délai q = 12. La première corrélation est toute proche de ne pas être significativement différente de 0, c'est pour ça que nous l"'ignorons" et identifions un processus MA.

- L'autocorrélogramme partiel nous montre plusieurs corrélations significativement de 0 qui s'ammortissent.

Nous allons maintenant estimer nos paramètres $\theta$ et $\Theta$

```{r}
# Estimation des paramètres
modelNt <- arima(Nt, order = c(0, 0, 1),seasonal=list(order=c(0,0,1),period=12))
# Récupération des valeurs numériques des paramètres estimés
theta.N <- as.numeric(modelNt$coef[1])
Theta.N <- as.numeric(modelNt$coef[2])
```

Nous estimons donc $\theta$ = `r round(theta.N, 4)` et $\Theta$ = `r round(Theta.N, 4)`

Nous cherchons maintenant à minimiser nos résidus, nous allons définir une fonction qui calcule les résidus du modèle. Puis, nous optimisons les paramètres de notre modèle pour minimiser les résidus.

```{r}
f <- function(v) {
  omega_0 <- v[1]
  omega_1 <- v[2]
  delta <- v[3]
  theta.N <- v[4]
  Theta.N <- v[5]
  bt<-filter(
    filter(dxt,
           filter=c(omega_0,omega_1),
           method='convolution'),
    filter=c(delta),
    method = "recursive")
  
  # at <- lag(dxt, -1) * omega_0
  # bt <- filter(at, filter = c(delta), method = "recursive")
  bt<-na.omit(bt)
  ct <- dyt - bt
  
  dt <- filter(ct, filter = c(-theta.N), method = "recursive")
  dt <- filter(dt, filter=c(rep(0,11),-Theta.N),method="recursive",init=rep(0,12))
  SS <- sum(dt ^ 2, na.rm = TRUE)
  return(SS)
}
v <- c(omega_0 = omega_0,omega_1=omega_1, delta = delta_1, theta.N = theta.N, Theta.N=Theta.N)

par.estim <- optim(v, f, method = "BFGS", hessian = TRUE)
```

Le modèle optimal a les paramètres suivants : {$\omega_0$ = `r round(par.estim$par[1],4)`, $\omega_1$ = `r round(par.estim$par[2],4)`, $\delta_1$ = `r round(par.estim$par[3],4)` $\theta$ = `r round(par.estim$par[4],4)` et $\Theta$ = `r round(par.estim$par[5],4)`}

```{r}
omega_0 <- par.estim$par[1]
omega_1 <- par.estim$par[2]
delta <- par.estim$par[3]
theta.N <- par.estim$par[4]
Theta.N <- par.estim$par[5]

bt <- filter(filter(dxt,filter=c(omega_0,omega_1),method = 'convolution'), filter = c(delta), method = "recursive")
ct <- dyt - bt
dt <- filter(ct, filter = c(-theta.N), method = "recursive")
dt <- filter(dt, filter=c(rep(0,11),-Theta.N),method="recursive",init=rep(0,12))
# Estimation de la variance des epsilon_t = dt
S2 <- sum(dt ^ 2, na.rm = TRUE) / length(dt)
```

Nous pouvons maintenant modéliser notre série avec les paramètres optimaux obtenus. Nous avons avec ce modèle, une erreur standard moyenne de `r round(S2,4)`

```{r}
par(mfrow=c(1,2), mar=c(5,4,4,2)+0.1)
# Vérification que epsilon_t = dt est un bruit blanc
acf(dt, na.action = na.pass, ci.type = "ma",main="Résidus finaux")
pacf(dt, na.action = na.pass, ylim=c(-0.2,1),main="Résidus finaux")
```

Nous voyons grâce aux autocorrélogrammes simple et partiel qu'avec ce modèle, les résidus n'ont plus d'auto-corrélations significativement différentes de 0. Les résidus sont du bruit blanc.

```{r}
par(mar=c(5,5,4,2)+0.1)
crossCorr <-
  ccf(
    dxt,
    na.omit(dt),
    ylim = c(-0.25, 0.25),
    main = "Ventes -> Résidus finaux",
    ylab = expression(italic(hat(rho)[alpha * epsilon](k))),
    xlab = expression(italic(k)),
    las = 1,
    frame = FALSE
  )
```

Nous n'observons pas de délai où les résidus sont significativement corrélés à la série d'entrée (les ventes). Notre modèle est donc valide.

# 3. Consommation d'alcool et taux de mortalité par cirrhose.
```{r}
rm(list=ls())
require(astsa)
dataSet <- read.table("Data/mortAlcool",header=TRUE)

xt <- ts(dataSet[,3],start=1945) # Consommation d'alcool
yt <- ts(dataSet[,2],start=1945) # Mortalité par cirrhose
```


```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1) + 0.1)
plot.ts(
  xt,
  las = 1,
  ylab = expression(italic(X[t])),
  xlab = expression(italic(t)),
  main = "Consommation d'alcool",
  frame = FALSE
)
plot.ts(
  yt ,
  las = 1,
  ylab = expression(italic(Y[t])),
  xlab = expression(italic(t)),
  main = "Mortalité liée à la cirrhose",
  frame = FALSE
)

corBrute <- cor(xt, yt)
```

On remarque une forte similarité entre le graphique de la conssommation d'alcool et la mortalité liée à la cirrhose où l'on voit sur les deux graphiques une décroissance régulière depuis la fin des années 1940. Cette décroissance s'accélère à la fin des années 1960 avec une baisse importante de la quantité de consommation d'alcool et dans le même intervalle de temps, on note aussi une baisse de mortalité liée à la cirrhose.  
Ces liens sont confirmés par une corrélation positive très signification(`r corBrute`) entre les deux séries.  

Etant données que les séries ne sont pas stationnaires, par la suite, nous travaillons avec $x_t = \nabla X_t$ et $y_t = \nabla Y_t$

```{r}
dxt <- diff(xt)
dyt <- diff(yt)

par(mfrow=c(1,2))
acf(dxt,lag.max = 40 )
pacf(dxt,lag.max = 40 )
```

L'ACF a une une décroissance exponentielle, de plus les autocorrélations empiriques ont une forme sinisoïdale dont seul le premier retard semble très significatif. On peut donc envisager un modèle **ARMA(1,1,1)** dont les coefficients sont donnés par le tableau suivant.

```{r}
xt_arma <- arima(xt,order=c(1,1,1),method="CSS")
pander::pander(xt_arma)
```

A partir de là, on peut envisager un filtrage de nos 2 séries avec les paramètres trouvés.


```{r}
phi <- xt_arma$coef[1]
theta <- xt_arma$coef[2]

at <- filter(dxt, filter = c(-theta), method = "recursive")
at <- na.omit(filter(at, filter=c(1, -phi), method = "convolution"))

bt <- filter(dyt, filter = c(-theta), method = "recursive")
bt <- na.omit(filter(bt, filter = c(1, -phi), method = "convolution"))

par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)
crossCorr <- ccf(at, bt)
abline(v=0, lty = 3, col="blue")
```

On a qu'un seul retard qui est significatif au retard 5. Nou avons donc l'équation suivante:

$$v(B) = \omega_0B^5$$

où les $v_k$ peuvent être estimés par:

$$v_k=\sum^r_{j=1}\delta_jv_{k-j}+\omega_0;\ \ k=b$$


pour b=5 nous avons donc :

$$
\omega_0=v_5 \ pour \ k=b=5
$$

```{r}
temp <- crossCorr$acf * sd(bt) / sd(at) 
omega_0 <- temp[9]

temp_t <- lag(dxt, k =  -5) * omega_0
Nt <- dyt - temp_t

par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 0.1)
acf(Nt,lag.max = 40)
pacf(Nt,lag.max = 40)
```

Sur l'Acf, on remarque un changement de signe sur les retards multiples de 12. De plus, sur le Pacf, seul le retard 1 est significatif. Nous pouvons donc modéliser avec un processus **ARMA(1,0,1)**.  
Les paramètres estimés sont données par le tableau suivant:

```{r}
Nt_arma <- arima(Nt, order = c(1, 0, 1))

pander::pander(Nt_arma)
```


On utilise la fonction utilisée dans l'exercice Housing pour calculer les résidus.

```{r}
# Définition de la fonction f qui calcule les résidus du modèle et
# qui retourne la somme des carrés qui elle, est à minimiser
f <- function(v) {
  omega_0 <- v[1]
  theta_Nt <- v[2]
  phi_Nt <- v[3]
  betha0<-v[4]
  bt<-lag(dxt, -5) * omega_0 + betha0
  
  # at <- lag(dxt, -1) * omega_0
  # bt <- filter(at, filter = c(delta), method = "recursive")
  bt<-na.omit(bt)
  ct <- dyt - bt
  
  dt <- filter(ct, filter = c(-theta_Nt), method = "recursive")
  dt <- filter(dt, filter=c(1,-phi_Nt),method="convolution")
  SS <- sum(dt ^ 2, na.rm = TRUE)
  return(SS)
}

phi_Nt <- as.numeric(Nt_arma$coef[1])
theta_Nt <- as.numeric(Nt_arma$coef[2])
betha0 <- as.numeric(Nt_arma$coef[3])

v <- c(omega_0 = omega_0, theta_Nt = theta_Nt, phi_Nt = phi_Nt, betha0 = betha0)
par_estim <- optim(v, f, method = "BFGS", hessian = TRUE)
```


```{r}
# Calcul des résidus du modèle final
omega_0 <- par_estim$par[1]
theta_Nt <- par_estim$par[2]
phi_Nt <- par_estim$par[3]
betha0 <- par_estim$par[4]
```

On a ensuite les paramètres optimaux:
```{r}
pander::pander(par_estim$par)
```

```{r}
bt <- lag(dxt, -5) * omega_0
ct <- dyt - (bt + betha0)

dt <- filter(ct, filter=c(-theta_Nt),method="recursive")
dt <- na.omit(filter(dt, filter=c(1,-phi_Nt),method="convolution"))

# Estimation de la variance des epsilon_t = dt
S2 <- sum(dt ^ 2, na.rm = TRUE) / length(dt)
```

Par la suite, nous avons calculé la somme au carré des erreurs qui est: `r S2`

```{r}
par(mfrow=c(1,2), mar=c(5,4,4,2)+0.1)
# Vérification que epsilon_t = dt est un bruit blanc
acf(dt, na.action = na.pass, ci.type = "ma",main="Résidus finaux")
pacf(dt, na.action = na.pass, ylim=c(-0.2,1),main="Résidus finaux")
```
```{r}
forecast::checkresiduals(dt)
```


Les résidus semblent stationnaires et suivent une loi normale. Aucune autocorrélation des erreurs n'est significative.

```{r}
par( mar=c(5,5,4,2)+0.1)
# Vérifiation que l'entrée et les résidus finaux ne sont pas corrélés.
crossCorr <-
  ccf(
    dxt,
    dt,
    # ylim = c(-0.25, 0.25),
    main = "Ventes -> Résidus finaux",
    ylab = expression(italic(hat(rho)[alpha * epsilon](k))),
    xlab = expression(italic(k)),
    las = 1,
    frame = FALSE
  )
```

Aucune autocorrélation n'est significative entre les variables prédictives et les résidus.  

Notre modèle final est donc le suivant :

$$
\nabla Y_t=0.058\times B^5+0.006+0.198\times B \times N_t+(1-1.116\times B)\epsilon_t
$$
  






